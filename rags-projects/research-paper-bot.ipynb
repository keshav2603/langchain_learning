{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04a7790",
   "metadata": {},
   "source": [
    "# Research Paper Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679f13c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb32811a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate,SystemMessagePromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c414f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-3-flash-preview\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3416a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'type': 'text', 'text': 'Hello! How can I help you today?', 'extras': {'signature': 'EqYCCqMCAb4+9vvgfmcZw5rJVCSnBYeU8I21ARq59n004Mjx9w+y23B82J7LTLZXaKou3AgYWkgl3Xun8Iv72DfoGw2pgiL/j+TCB5iejZO93xJWqPuuRlf9bPYkrs7mpIpHZ+ofUVmvddp1Lq3Nj2Lk5erQz665i+qa1yGr49fte8MhzVwSfEUQZqNg2F5/kE3k4B05eoTFM3+9rskF3PR1Cj9OJmYWDn2il2EPH3K3cJ9scf/J4HnhysuPKpqtjvBVLd40BXjtIXskSXezaBQPFL1G83W7GDXxMvwZMlJedowHfV/hzDTyzbbD+8vfvGrE/j00V92J4VfbIps9Yd9IMFo76P8hkNYos6LkldFP6S+TgfT1F5k22IFHOK4bNz37/gSF8Kb5'}}], additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-3-flash-preview', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c891a-f075-7ae0-8d19-d0c57e08d335-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 2, 'output_tokens': 73, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 65}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e6ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a70ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8584dfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'author': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='An Introduction to Convolutional Neural Networks\\nKeiron O’Shea1 and Ryan Nash2\\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\\nkeo7@aber.ac.uk\\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\\n4YW\\nnashrd@live.lancs.ac.uk\\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\\nbiologically inspired computational models are able to far exceed the per-\\nformance of previous forms of artiﬁcial intelligence in common machine\\nlearning tasks. One of the most impressive forms of ANN architecture is\\nthat of the Convolutional Neural Network (CNN). CNNs are primarily\\nused to solve difﬁcult image-driven pattern recognition tasks and with\\ntheir precise yet simple architecture, offers a simpliﬁed method of getting\\nstarted with ANNs.\\nThis document provides a brief introduction to CNNs, discussing recently\\npublished papers and newly formed techniques in developing these bril-\\nliantly fantastic image recognition models. This introduction assumes you\\nare familiar with the fundamentals of ANNs and machine learning.\\nKeywords: Pattern recognition, artiﬁcial neural networks, machine learn-\\ning, image analysis\\n1 Introduction\\nArtiﬁcial Neural Networks (ANNs) are computational processing systems of\\nwhich are heavily inspired by way biological nervous systems (such as the hu-\\nman brain) operate. ANNs are mainly comprised of a high number of intercon-\\nnected computational nodes (referred to as neurons), of which work entwine in\\na distributed fashion to collectively learn from the input in order to optimise its\\nﬁnal output.\\nThe basic structure of a ANN can be modelled as shown in Figure 1. We would\\nload the input, usually in the form of a multidimensional vector to the input\\nlayer of which will distribute it to the hidden layers. The hidden layers will then\\nmake decisions from the previous layer and weigh up how a stochastic change\\nwithin itself detriments or improves the ﬁnal output, and this is referred to as\\nthe process of learning. Having multiple hidden layers stacked upon each-other\\nis commonly called deep learning.\\narXiv:1511.08458v2  [cs.NE]  2 Dec 2015')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed54ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter= RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7be95a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=splitter.create_documents([text.page_content for text in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03839902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='An Introduction to Convolutional Neural Networks\\nKeiron O’Shea1 and Ryan Nash2\\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\\nkeo7@aber.ac.uk\\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\\n4YW\\nnashrd@live.lancs.ac.uk\\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\\nbiologically inspired computational models are able to far exceed the per-\\nformance of previous forms of artiﬁcial intelligence in common machine\\nlearning tasks. One of the most impressive forms of ANN architecture is\\nthat of the Convolutional Neural Network (CNN). CNNs are primarily\\nused to solve difﬁcult image-driven pattern recognition tasks and with\\ntheir precise yet simple architecture, offers a simpliﬁed method of getting\\nstarted with ANNs.\\nThis document provides a brief introduction to CNNs, discussing recently'),\n",
       " Document(metadata={}, page_content='their precise yet simple architecture, offers a simpliﬁed method of getting\\nstarted with ANNs.\\nThis document provides a brief introduction to CNNs, discussing recently\\npublished papers and newly formed techniques in developing these bril-\\nliantly fantastic image recognition models. This introduction assumes you\\nare familiar with the fundamentals of ANNs and machine learning.\\nKeywords: Pattern recognition, artiﬁcial neural networks, machine learn-\\ning, image analysis\\n1 Introduction\\nArtiﬁcial Neural Networks (ANNs) are computational processing systems of\\nwhich are heavily inspired by way biological nervous systems (such as the hu-\\nman brain) operate. ANNs are mainly comprised of a high number of intercon-\\nnected computational nodes (referred to as neurons), of which work entwine in\\na distributed fashion to collectively learn from the input in order to optimise its\\nﬁnal output.\\nThe basic structure of a ANN can be modelled as shown in Figure 1. We would'),\n",
       " Document(metadata={}, page_content='a distributed fashion to collectively learn from the input in order to optimise its\\nﬁnal output.\\nThe basic structure of a ANN can be modelled as shown in Figure 1. We would\\nload the input, usually in the form of a multidimensional vector to the input\\nlayer of which will distribute it to the hidden layers. The hidden layers will then\\nmake decisions from the previous layer and weigh up how a stochastic change\\nwithin itself detriments or improves the ﬁnal output, and this is referred to as\\nthe process of learning. Having multiple hidden layers stacked upon each-other\\nis commonly called deep learning.\\narXiv:1511.08458v2  [cs.NE]  2 Dec 2015'),\n",
       " Document(metadata={}, page_content='2 Keiron O’Shea et al.\\nInput 1\\nInput 2\\nInput 3\\nInput 4\\nInput Layer Hidden Layer Output Layer\\nOutput\\nFig. 1: A simple three layered feedforward neural network (FNN), comprised\\nof a input layer, a hidden layer and an output layer. This structure is the basis\\nof a number of common ANN architectures, included but not limited to Feed-\\nforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and\\nRecurrent Neural Networks (RNNs).\\nThe two key learning paradigms in image processing tasks are supervised and\\nunsupervised learning. Supervised learning is learning through pre-labelled\\ninputs, which act as targets. For each training example there will be a set of\\ninput values (vectors) and one or more associated designated output values.\\nThe goal of this form of training is to reduce the models overall classiﬁcation\\nerror, through correct calculation of the output value of training example by\\ntraining.\\nUnsupervised learning differs in that the training set does not include any la-'),\n",
       " Document(metadata={}, page_content='error, through correct calculation of the output value of training example by\\ntraining.\\nUnsupervised learning differs in that the training set does not include any la-\\nbels. Success is usually determined by whether the network is able to reduce or\\nincrease an associated cost function. However, it is important to note that most\\nimage-focused pattern-recognition tasks usually depend on classiﬁcation using\\nsupervised learning.\\nConvolutional Neural Networks (CNNs) are analogous to traditional ANNs\\nin that they are comprised of neurons that self-optimise through learning. Each\\nneuron will still receive an input and perform a operation (such as a scalar\\nproduct followed by a non-linear function) - the basis of countless ANNs. From\\nthe input raw image vectors to the ﬁnal output of the class score, the entire of\\nthe network will still express a single perceptive score function (the weight).\\nThe last layer will contain loss functions associated with the classes, and all of'),\n",
       " Document(metadata={}, page_content='the network will still express a single perceptive score function (the weight).\\nThe last layer will contain loss functions associated with the classes, and all of\\nthe regular tips and tricks developed for traditional ANNs still apply.\\nThe only notable difference between CNNs and traditional ANNs is that CNNs\\nare primarily used in the ﬁeld of pattern recognition within images. This allows\\nus to encode image-speciﬁc features into the architecture, making the network'),\n",
       " Document(metadata={}, page_content='Introduction to Convolutional Neural Networks 3\\nmore suited for image-focused tasks - whilst further reducing the parameters\\nrequired to set up the model.\\nOne of the largest limitations of traditional forms of ANN is that they tend to\\nstruggle with the computational complexity required to compute image data.\\nCommon machine learning benchmarking datasets such as the MNIST database\\nof handwritten digits are suitable for most forms of ANN, due to its relatively\\nsmall image dimensionality of just 28 × 28. With this dataset a single neuron in\\nthe ﬁrst hidden layer will contain 784 weights (28×28×1 where 1 bare in mind\\nthat MNIST is normalised to just black and white values), which is manageable\\nfor most forms of ANN.\\nIf you consider a more substantial coloured image input of64 × 64, the number\\nof weights on just a single neuron of the ﬁrst layer increases substantially to\\n12, 288. Also take into account that to deal with this scale of input, the network'),\n",
       " Document(metadata={}, page_content='of weights on just a single neuron of the ﬁrst layer increases substantially to\\n12, 288. Also take into account that to deal with this scale of input, the network\\nwill also need to be a lot larger than one used to classify colour-normalised\\nMNIST digits, then you will understand the drawbacks of using such models.\\n1.1 Overﬁtting\\nBut why does it matter? Surely we could just increase the number of hidden lay-\\ners in our network, and perhaps increase the number of neurons within them?\\nThe simple answer to this question is no. This is down to two reasons, one be-\\ning the simple problem of not having unlimited computational power and time\\nto train these huge ANNs.\\nThe second reason is stopping or reducing the effects of overﬁtting.Overﬁtting\\nis basically when a network is unable to learn effectively due to a number of\\nreasons. It is an important concept of most, if not all machine learning algo-\\nrithms and it is important that every precaution is taken as to reduce its effects.'),\n",
       " Document(metadata={}, page_content='reasons. It is an important concept of most, if not all machine learning algo-\\nrithms and it is important that every precaution is taken as to reduce its effects.\\nIf our models were to exhibit signs of overﬁtting then we may see a reduced\\nability to pinpoint generalised features for not only our training dataset, but\\nalso our test and prediction sets.\\nThis is the main reason behind reducing the complexity of our ANNs. The less\\nparameters required to train, the less likely the network will overﬁt - and of\\ncourse, improve the predictive performance of the model.\\n2 CNN architecture\\nAs noted earlier, CNNs primarily focus on the basis that the input will be com-\\nprised of images. This focuses the architecture to be set up in way to best suit\\nthe need for dealing with the speciﬁc type of data.'),\n",
       " Document(metadata={}, page_content='4 Keiron O’Shea et al.\\nOne of the key differences is that the neurons that the layers within the CNN\\nare comprised of neurons organised into three dimensions, the spatial dimen-\\nsionality of the input (height and the width) and the depth. The depth does not\\nrefer to the total number of layers within the ANN, but the third dimension of a\\nactivation volume. Unlike standard ANNS, the neurons within any given layer\\nwill only connect to a small region of the layer preceding it.\\nIn practice this would mean that for the example given earlier, the input ’vol-\\nume’ will have a dimensionality of64 × 64 × 3 (height, width and depth), lead-\\ning to a ﬁnal output layer comprised of a dimensionality of 1 × 1 × n (where\\nn represents the possible number of classes) as we would have condensed the\\nfull input dimensionality into a smaller volume of class scores ﬁled across the\\ndepth dimension.\\n2.1 Overall architecture\\nCNNs are comprised of three types of layers. These are convolutional layers,'),\n",
       " Document(metadata={}, page_content='full input dimensionality into a smaller volume of class scores ﬁled across the\\ndepth dimension.\\n2.1 Overall architecture\\nCNNs are comprised of three types of layers. These are convolutional layers,\\npooling layers and fully-connected layers . When these layers are stacked, a\\nCNN architecture has been formed. A simpliﬁed CNN architecture for MNIST\\nclassiﬁcation is illustrated in Figure 2.\\ninput\\n0\\n9\\nconvolution\\n w/ReLu pooling\\noutput \\nfully-connected\\nw/ ReLu\\nfully-connected\\n...\\nFig. 2: An simple CNN architecture, comprised of just ﬁve layers\\nThe basic functionality of the example CNN above can be broken down into\\nfour key areas.\\n1. As found in other forms of ANN, the input layer will hold the pixel values\\nof the image.\\n2. The convolutional layer will determine the output of neurons of which are\\nconnected to local regions of the input through the calculation of the scalar\\nproduct between their weights and the region connected to the input vol-'),\n",
       " Document(metadata={}, page_content='connected to local regions of the input through the calculation of the scalar\\nproduct between their weights and the region connected to the input vol-\\nume. The rectiﬁed linear unit (commonly shortened to ReLu) aims to apply'),\n",
       " Document(metadata={}, page_content='Introduction to Convolutional Neural Networks 5\\nan ’elementwise’ activation function such as sigmoid to the output of the\\nactivation produced by the previous layer.\\n3. The pooling layer will then simply perform downsampling along the spa-\\ntial dimensionality of the given input, further reducing the number of pa-\\nrameters within that activation.\\n4. The fully-connected layers will then perform the same duties found in\\nstandard ANNs and attempt to produce class scores from the activations,\\nto be used for classiﬁcation. It is also suggested that ReLu may be used\\nbetween these layers, as to improve performance.\\nThrough this simple method of transformation, CNNs are able to transform\\nthe original input layer by layer using convolutional and downsampling tech-\\nniques to produce class scores for classiﬁcation and regression purposes.\\nFig. 3: Activations taken from the ﬁrst convolutional layer of a simplistic deep\\nCNN, after training on the MNIST database of handwritten digits. If you look'),\n",
       " Document(metadata={}, page_content='Fig. 3: Activations taken from the ﬁrst convolutional layer of a simplistic deep\\nCNN, after training on the MNIST database of handwritten digits. If you look\\ncarefully, you can see that the network has successfully picked up on character-\\nistics unique to speciﬁc numeric digits.\\nHowever, it is important to note that simply understanding the overall archi-\\ntecture of a CNN architecture will not sufﬁce. The creation and optimisation\\nof these models can take quite some time, and can be quite confusing. We will\\nnow explore in detail the individual layers, detailing their hyperparameters\\nand connectivities.\\n2.2 Convolutional layer\\nAs the name implies, the convolutional layer plays a vital role in how CNNs\\noperate. The layers parameters focus around the use of learnable kernels.'),\n",
       " Document(metadata={}, page_content='6 Keiron O’Shea et al.\\nThese kernels are usually small in spatial dimensionality, but spreads along the\\nentirety of the depth of the input. When the data hits a convolutional layer,\\nthe layer convolves each ﬁlter across the spatial dimensionality of the input to\\nproduce a 2D activation map. These activation maps can be visualised, as seen\\nin Figure 3.\\nAs we glide through the input, the scalar product is calculated for each value in\\nthat kernel. (Figure 4) From this the network will learn kernels that ’ﬁre’ when\\nthey see a speciﬁc feature at a given spatial position of the input. These are\\ncommonly known as activations.\\n0 0\\n0 1\\n0\\n2\\n0 1 1\\n4 0\\n0 0\\n0\\n0\\n0 0 -4\\n-8\\nPooled Vector Kernel Destination Pixel0 0\\n0 1\\n0\\n2\\n0 1 1\\n0 0\\n1 1\\n0\\n2\\n1 1 1\\n1 0\\n0 0\\n0\\n1\\n0 1 1\\n0 0\\n1 1\\n0\\n0\\n1 1 1\\nInput Vector\\nFig. 4: A visual representation of a convolutional layer. The centre element of the\\nkernel is placed over the input vector, of which is then calculated and replaced'),\n",
       " Document(metadata={}, page_content='0 0\\n1 1\\n0\\n0\\n1 1 1\\nInput Vector\\nFig. 4: A visual representation of a convolutional layer. The centre element of the\\nkernel is placed over the input vector, of which is then calculated and replaced\\nwith a weighted sum of itself and any nearby pixels.\\nEvery kernel will have a corresponding activation map, of which will be stacked\\nalong the depth dimension to form the full output volume from the convolu-\\ntional layer.\\nAs we alluded to earlier, training ANNs on inputs such as images results in\\nmodels of which are too big to train effectively. This comes down to the fully-\\nconnected manner of standard ANN neurons, so to mitigate against this every\\nneuron in a convolutional layer is only connected to small region of the input\\nvolume. The dimensionality of this region is commonly referred to as the re-\\nceptive ﬁeld size of the neuron. The magnitude of the connectivity through the\\ndepth is nearly always equal to the depth of the input.'),\n",
       " Document(metadata={}, page_content='ceptive ﬁeld size of the neuron. The magnitude of the connectivity through the\\ndepth is nearly always equal to the depth of the input.\\nFor example, if the input to the network is an image of size 64 × 64 × 3 (a RGB-\\ncoloured image with a dimensionality of 64 × 64) and we set the receptive ﬁeld\\nsize as 6 × 6, we would have a total of 108 weights on each neuron within the\\nconvolutional layer. (6 × 6 × 3 where 3 is the magnitude of connectivity across\\nthe depth of the volume) To put this into perspective, a standard neuron seen\\nin other forms of ANN would contain 12, 288 weights each.\\nConvolutional layers are also able to signiﬁcantly reduce the complexity of the\\nmodel through the optimisation of its output. These are optimised through\\nthree hyperparameters, the depth, the stride and setting zero-padding.'),\n",
       " Document(metadata={}, page_content='Introduction to Convolutional Neural Networks 7\\nThe depth of the output volume produced by the convolutional layers can be\\nmanually set through the number of neurons within the layer to a the same\\nregion of the input. This can be seen with other forms of ANNs, where the\\nall of the neurons in the hidden layer are directly connected to every single\\nneuron beforehand. Reducing this hyperparameter can signiﬁcantly minimise\\nthe total number of neurons of the network, but it can also signiﬁcantly reduce\\nthe pattern recognition capabilities of the model.\\nWe are also able to deﬁne thestride in which we set the depth around the spatial\\ndimensionality of the input in order to place the receptive ﬁeld. For example if\\nwe were to set a stride as 1, then we would have a heavily overlapped receptive\\nﬁeld producing extremely large activations. Alternatively, setting the stride to a\\ngreater number will reduce the amount of overlapping and produce an output\\nof lower spatial dimensions.'),\n",
       " Document(metadata={}, page_content='ﬁeld producing extremely large activations. Alternatively, setting the stride to a\\ngreater number will reduce the amount of overlapping and produce an output\\nof lower spatial dimensions.\\nZero-padding is the simple process of padding the border of the input, and\\nis an effective method to give further control as to the dimensionality of the\\noutput volumes.\\nIt is important to understand that through using these techniques, we will alter\\nthe spatial dimensionality of the convolutional layers output. To calculate this,\\nyou can make use of the following formula:\\n(V − R) + 2Z\\nS + 1\\nWhere V represents the input volume size (height×width×depth), R represents\\nthe receptive ﬁeld size, Z is the amount of zero padding set and S referring to\\nthe stride. If the calculated result from this equation is not equal to a whole\\ninteger then the stride has been incorrectly set, as the neurons will be unable to\\nﬁt neatly across the given input.'),\n",
       " Document(metadata={}, page_content='the stride. If the calculated result from this equation is not equal to a whole\\ninteger then the stride has been incorrectly set, as the neurons will be unable to\\nﬁt neatly across the given input.\\nDespite our best efforts so far we will still ﬁnd that our models are still enor-\\nmous if we use an image input of any real dimensionality. However, methods\\nhave been developed as to greatly curtail the overall number of parameters\\nwithin the convolutional layer.\\nParameter sharing works on the assumption that if one region feature is useful\\nto compute at a set spatial region, then it is likely to be useful in another region.\\nIf we constrain each individual activation map within the output volume to the\\nsame weights and bias, then we will see a massive reduction in the number of\\nparameters being produced by the convolutional layer.\\nAs a result of this as the backpropagation stage occurs, each neuron in the out-\\nput will represent the overall gradient of which can be totalled across the depth'),\n",
       " Document(metadata={}, page_content='As a result of this as the backpropagation stage occurs, each neuron in the out-\\nput will represent the overall gradient of which can be totalled across the depth\\n- thus only updating a single set of weights, as opposed to every single one.'),\n",
       " Document(metadata={}, page_content='8 Keiron O’Shea et al.\\n2.3 Pooling layer\\nPooling layers aim to gradually reduce the dimensionality of the representa-\\ntion, and thus further reduce the number of parameters and the computational\\ncomplexity of the model.\\nThe pooling layer operates over each activation map in the input, and scales\\nits dimensionality using the “MAX” function. In most CNNs, these come in the\\nform of max-pooling layers with kernels of a dimensionality of 2 × 2 applied\\nwith a stride of 2 along the spatial dimensions of the input. This scales the\\nactivation map down to 25% of the original size - whilst maintaining the depth\\nvolume to its standard size.\\nDue to the destructive nature of the pooling layer, there are only two generally\\nobserved methods of max-pooling. Usually, the stride and ﬁlters of the pooling\\nlayers are both set to 2 × 2, which will allow the layer to extend through the\\nentirety of the spatial dimensionality of the input. Furthermore overlapping'),\n",
       " Document(metadata={}, page_content='layers are both set to 2 × 2, which will allow the layer to extend through the\\nentirety of the spatial dimensionality of the input. Furthermore overlapping\\npooling may be utilised, where the stride is set to 2 with a kernel size set to\\n3. Due to the destructive nature of pooling, having a kernel size above 3 will\\nusually greatly decrease the performance of the model.\\nIt is also important to understand that beyond max-pooling, CNN architectures\\nmay contain general-pooling. General pooling layers are comprised of pooling\\nneurons that are able to perform a multitude of common operations including\\nL1/L2-normalisation, and average pooling. However, this tutorial will primar-\\nily focus on the use of max-pooling.\\n2.4 Fully-connected layer\\nThe fully-connected layer contains neurons of which are directly connected to\\nthe neurons in the two adjacent layers, without being connected to any layers\\nwithin them. This is analogous to way that neurons are arranged in traditional'),\n",
       " Document(metadata={}, page_content='the neurons in the two adjacent layers, without being connected to any layers\\nwithin them. This is analogous to way that neurons are arranged in traditional\\nforms of ANN. (Figure 1)\\n3 Recipes\\nDespite the relatively small number of layers required to form a CNN, there\\nis no set way of formulating a CNN architecture. That being said, it would be\\nidiotic to simply throw a few of layers together and expect it to work. Through\\nreading of related literature it is obvious that much like other forms of ANNs,\\nCNNs tend to follow a common architecture. This common architecture is illus-\\ntrated in Figure 2, where convolutional layers are stacked, followed by pooling\\nlayers in a repeated manner before feeding forward to fully-connected layers.'),\n",
       " Document(metadata={}, page_content='Introduction to Convolutional Neural Networks 9\\nAnother common CNN architecture is to stack two convolutional layers before\\neach pooling layer, as illustrated in Figure 5. This is strongly recommended as\\nstacking multiple convolutional layers allows for more complex features of the\\ninput vector to be selected.\\ninput\\nconvolution w/ ReLu pooling\\nconvolution\\nw/ ReLu\\npooling\\nfully-connected\\nw/ ReLu\\nfully-connected\\nconvolution w/ ReLu pooling\\n0\\n9\\noutput \\n...\\nFig. 5: A common form of CNN architecture in which convolutional layers are\\nstacked between ReLus continuously before being passed through the pooling\\nlayer, before going between one or many fully connected ReLus.\\nIt is also advised to split large convolutional layers up into many smaller sized\\nconvolutional layers. This is to reduce the amount of computational complexity\\nwithin a given convolutional layer. For example, if you were to stack three con-\\nvolutional layers on top of each other with a receptive ﬁeld of3×3. Each neuron'),\n",
       " Document(metadata={}, page_content='within a given convolutional layer. For example, if you were to stack three con-\\nvolutional layers on top of each other with a receptive ﬁeld of3×3. Each neuron\\nof the ﬁrst convolutional layer will have a 3 ×3 view of the input vector. A neu-\\nron on the second convolutional layer will then have a 5 × 5 view of the input\\nvector. A neuron on the third convolutional layer will then have a7 × 7 view of\\nthe input vector. As these stacks feature non-linearities which in turn allows us\\nto express stronger features of the input with fewer parameters. However, it is\\nimportant to understand that this does come with a distinct memory allocation\\nproblem - especially when making use of the backpropagation algorithm.\\nThe input layer should be recursively divisible by two. Common numbers in-\\nclude 32 × 32, 64 × 64, 96 × 96, 128 × 128 and 224 × 224.\\nWhilst using small ﬁlters, set stride to one and make use of zero-padding as to'),\n",
       " Document(metadata={}, page_content='clude 32 × 32, 64 × 64, 96 × 96, 128 × 128 and 224 × 224.\\nWhilst using small ﬁlters, set stride to one and make use of zero-padding as to\\nensure that the convolutional layers do not reconﬁgure any of the dimension-\\nality of the input. The amount of zero-padding to be used should be calculated\\nby taking one away from the receptive ﬁeld size and dividing by two.activation\\nCNNs are extremely powerful machine learning algorithms, however they can\\nbe horrendously resource-heavy. An example of this problem could be in ﬁlter-\\ning a large image (anything over 128 × 128 could be considered large), so if the\\ninput is 227 × 227 (as seen with ImageNet) and we’re ﬁltering with 64 kernels\\neach with a zero padding of then the result will be three activation vectors of\\nsize 227 × 227 × 64 - which calculates to roughly 10 million activations - or an\\nenormous 70 megabytes of memory per image. In this case you have two op-\\ntions. Firstly, you can reduce the spatial dimensionality of the input images by'),\n",
       " Document(metadata={}, page_content='10 Keiron O’Shea et al.\\nresizing the raw images to something a little less heavy. Alternatively, you can\\ngo against everything we stated earlier in this document and opt for larger ﬁlter\\nsizes with a larger stride (2, as opposed to 1).\\nIn addition to the few rules-of-thumb outlined above, it is also important to ac-\\nknowledge a few ’tricks’ about generalised ANN training techniques. The au-\\nthors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training\\nRestricted Boltzmann Machines”.\\n4 Conclusion\\nConvolutional Neural Networks differ to other forms of Artiﬁcal Neural Net-\\nwork in that instead of focusing on the entirety of the problem domain, knowl-\\nedge about the speciﬁc type of input is exploited. This in turn allows for a much\\nsimpler network architecture to be set up.\\nThis paper has outlined the basic concepts of Convolutional Neural Networks,\\nexplaining the layers required to build one and detailing how best to structure\\nthe network in most image analysis tasks.'),\n",
       " Document(metadata={}, page_content='This paper has outlined the basic concepts of Convolutional Neural Networks,\\nexplaining the layers required to build one and detailing how best to structure\\nthe network in most image analysis tasks.\\nResearch in the ﬁeld of image analysis using neural networks has somewhat\\nslowed in recent times. This is partly due to the incorrect belief surrounding the\\nlevel of complexity and knowledge required to begin modelling these superbly\\npowerful machine learning algorithms. The authors hope that this paper has\\nin some way reduced this confusion, and made the ﬁeld more accessible to\\nbeginners.\\nAcknowledgements\\nThe authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for\\nuseful discussion and suggestions.\\nReferences\\n1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for im-\\nage classiﬁcation. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE\\nConference on. pp. 3642–3649. IEEE (2012)'),\n",
       " Document(metadata={}, page_content='age classiﬁcation. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE\\nConference on. pp. 3642–3649. IEEE (2012)\\n2. Cires ¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in\\nbreast cancer histology images with deep neural networks. In: Medical Image Com-\\nputing and Computer-Assisted Intervention–MICCAI 2013, pp. 411–418. Springer\\n(2013)\\n3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible,\\nhigh performance convolutional neural networks for image classiﬁcation. In: IJCAI\\nProceedings-International Joint Conference on Artiﬁcial Intelligence. vol. 22, p. 1237\\n(2011)'),\n",
       " Document(metadata={}, page_content='Introduction to Convolutional Neural Networks 11\\n4. Cires ¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural\\nnetwork committees for handwritten character classiﬁcation. In: Document Analysis\\nand Recognition (ICDAR), 2011 International Conference on. pp. 1135–1139. IEEE\\n(2011)\\n5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-\\nworksa review. Pattern recognition 35(10), 2279–2301 (2002)\\n6. Farabet, C., Martini, B., Akselrod, P ., Talay, S., LeCun, Y., Culurciello, E.: Hardware\\naccelerated convolutional neural networks for synthetic vision systems. In: Circuits\\nand Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp.\\n257–260. IEEE (2010)\\n7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum\\n9(1), 926 (2010)\\n8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Im-\\nproving neural networks by preventing co-adaptation of feature detectors. arXiv'),\n",
       " Document(metadata={}, page_content='9(1), 926 (2010)\\n8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Im-\\nproving neural networks by preventing co-adaptation of feature detectors. arXiv\\npreprint arXiv:1207.0580 (2012)\\n9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action\\nrecognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1),\\n221–231 (2013)\\n10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-\\nscale video classiﬁcation with convolutional neural networks. In: Computer Vision\\nand Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725–1732. IEEE\\n(2014)\\n11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep convo-\\nlutional neural networks. In: Advances in neural information processing systems.\\npp. 1097–1105 (2012)\\n12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,'),\n",
       " Document(metadata={}, page_content='lutional neural networks. In: Advances in neural information processing systems.\\npp. 1097–1105 (2012)\\n12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,\\nL.D.: Backpropagation applied to handwritten zip code recognition. Neural compu-\\ntation 1(4), 541–551 (1989)\\n13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P .: Gradient-based learning applied to doc-\\nument recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\\n14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition.\\nNeural Networks, IEEE Transactions on 9(4), 685–696 (1998)\\n15. Simard, P .Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural net-\\nworks applied to visual document analysis. In: null. p. 958. IEEE (2003)\\n16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of\\nToronto (2013)\\n17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with con-'),\n",
       " Document(metadata={}, page_content='16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of\\nToronto (2013)\\n17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with con-\\nvolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings.\\nIEEE. pp. 224–229. IEEE (2005)\\n18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In:\\nAdvances in Neural Information Processing Systems. pp. 2553–2561 (2013)\\n19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks\\n(siconnets) and their application of face detection. In: Neural Networks, 2003. Pro-\\nceedings of the International Joint Conference on. vol. 3, pp. 2157–2162. IEEE (2003)\\n20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional\\nneural networks. arXiv preprint arXiv:1301.3557 (2013)\\n21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In:'),\n",
       " Document(metadata={}, page_content='neural networks. arXiv preprint arXiv:1301.3557 (2013)\\n21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In:\\nComputer Vision–ECCV 2014, pp. 818–833. Springer (2014)')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfa47d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cf0f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model =HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "628bf490",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store=Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    collection_name=\"sampll1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4598242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2c613852-594d-4e3a-804a-be7eefc2b84c',\n",
       " 'ebef5252-b134-41ca-bcb6-8e97141af5f1',\n",
       " '1ad0f894-d558-4296-a8bc-3a5696569ba7',\n",
       " 'ab90d10c-0229-40c6-afec-4cf64fcdc22f',\n",
       " 'd65b28b2-b99b-4a3e-b7b0-bbb3e7dbda44',\n",
       " '32b2960c-34c4-4878-a18d-c78282f3bdcd',\n",
       " '0376e084-5f2a-483b-8dbb-4273523c14e4',\n",
       " '6e69ae4e-c60f-482c-900c-72b9a6e7d4f9',\n",
       " '5a44ee30-7697-46b1-85e2-5b7a052d80fe',\n",
       " 'b79d506f-c9c7-4005-8b58-50f25822cffb',\n",
       " '8fe3f93a-7f94-41ba-9fbe-94ebe78b6476',\n",
       " 'b9b5bb28-a9fb-45d0-9763-1072eadba957',\n",
       " '7fda893a-5444-48e0-8562-f19c29a19621',\n",
       " 'b9a8a416-cda6-4e40-878a-a4d45781dd04',\n",
       " '00b11a99-9162-4441-aed8-77bf5a271f79',\n",
       " '607e2f67-6111-4e05-9067-4ef1d9b38aa5',\n",
       " '374107fd-5079-45b9-9bc2-768077493730',\n",
       " '72672bf2-6a39-48b8-9265-d20431c04029',\n",
       " '08c00980-59f9-42ed-8230-b88c0517f1f1',\n",
       " '2f7bac12-9650-468b-817a-94e955d4b2eb',\n",
       " '1865c0d0-26d5-4511-a461-cc1d768117ae',\n",
       " '09fcbe70-d19d-43fe-a094-60f6e3db46f0',\n",
       " 'e84da5c2-dab2-4119-a3fb-d92de6d12aff',\n",
       " '0a75f713-d517-4a96-8571-5e8dc1dab8d9',\n",
       " '90efd6ba-2db0-4550-9226-fa3af24a20cf',\n",
       " '69311ee2-1a3e-44ef-9153-10975b1d0d05',\n",
       " '0f665137-8cfe-4140-84c5-fcda4d73a3f9',\n",
       " '2efe71b7-f35a-4a7c-b5cc-3ca223d0b2c3',\n",
       " '0ad98aae-f92f-4151-8006-f5acea8c5422',\n",
       " '08187787-e21c-4a9c-8f7b-425117d0c6df',\n",
       " '9d778786-81b8-4c84-9fbb-97b69ca614db',\n",
       " '8cebe542-5682-43f6-a02a-3fb4472ec797',\n",
       " '11e3abee-cb9e-4737-8ceb-b3255c757259',\n",
       " 'f5738338-5102-40f8-9a8a-a253cf089f8a',\n",
       " 'ec2f721f-5f4b-46d2-bbb2-31074cce5992']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "657af1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievers=vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990935e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SystemMessagePromptTemplate\nprompt\n  Field required [type=missing, input_value={'template': ' \\nYou are ...ically reliable.\\n\\n\\n'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m prompt= \u001b[43mSystemMessagePromptTemplate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[33;43mYou are an expert AI Research Assistant specialized in explaining academic research papers clearly and accurately.\u001b[39;49m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[33;43mYour task is to answer questions strictly using the provided CONTEXT from the research paper.\u001b[39;49m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[33;43mRULES:\u001b[39;49m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;43m1. Use ONLY the information present in the provided CONTEXT.\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[33;43m2. If the answer is not explicitly supported by the context, say:\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[33;43m   \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThe provided context does not contain enough information to answer this.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[33;43m3. Do NOT fabricate missing details.\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[33;43m6. Keep explanations structured and logically clear.\u001b[39;49m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;43mEXPLANATION MODES:\u001b[39;49m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;43mIf the user does not specify a level of depth:\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[33;43m- Provide a clear, structured explanation.\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[33;43m- First give a short summary.\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[33;43m- Then provide a detailed breakdown.\u001b[39;49m\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m \u001b[33;43mIf the user asks to explain simply:\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[33;43m- Explain in beginner-friendly language.\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[33;43m- Avoid jargon.\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[33;43m- Use intuitive examples where possible.\u001b[39;49m\n\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[33;43mIf the user asks for technical detail:\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[33;43m- Use precise terminology.\u001b[39;49m\n\u001b[32m     29\u001b[39m \u001b[33;43m- Include equations if present in the context.\u001b[39;49m\n\u001b[32m     30\u001b[39m \u001b[33;43m- Explain variables clearly.\u001b[39;49m\n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[33;43mIf the user asks about implementation:\u001b[39;49m\n\u001b[32m     33\u001b[39m \u001b[33;43m- Translate methodology into step-by-step algorithmic logic.\u001b[39;49m\n\u001b[32m     34\u001b[39m \u001b[33;43m- Provide pseudocode if possible (only based on context).\u001b[39;49m\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m \u001b[33;43mMATH HANDLING:\u001b[39;49m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;43mIf equations appear:\u001b[39;49m\n\u001b[32m     39\u001b[39m \u001b[33;43m- Rewrite them in readable format.\u001b[39;49m\n\u001b[32m     40\u001b[39m \u001b[33;43m- Explain what each term represents.\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[33;43m- Explain the intuition behind the equation.\u001b[39;49m\n\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m \u001b[33;43mCOMPARISON HANDLING:\u001b[39;49m\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m \u001b[33;43mIf the user asks to compare with another method:\u001b[39;49m\n\u001b[32m     46\u001b[39m \u001b[33;43m- Only compare if both are mentioned in the context.\u001b[39;49m\n\u001b[32m     47\u001b[39m \u001b[33;43m- Otherwise say that comparison is not available in the provided document.\u001b[39;49m\n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[33;43mFORMAT:\u001b[39;49m\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m \u001b[33;43mAnswer Structure:\u001b[39;49m\n\u001b[32m     52\u001b[39m \u001b[33;43m1. Short Summary\u001b[39;49m\n\u001b[32m     53\u001b[39m \u001b[33;43m2. Detailed Explanation\u001b[39;49m\n\u001b[32m     54\u001b[39m \u001b[33;43m3. Key Points (bullet list if useful)\u001b[39;49m\n\u001b[32m     55\u001b[39m \u001b[33;43m4. Citations (Source: Page X)\u001b[39;49m\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[33;43mBe precise, grounded, and academically reliable.\u001b[39;49m\n\u001b[32m     58\u001b[39m \n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-ML/langchain-fundamentals/venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:117\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-ML/langchain-fundamentals/venv/lib/python3.13/site-packages/pydantic/main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for SystemMessagePromptTemplate\nprompt\n  Field required [type=missing, input_value={'template': ' \\nYou are ...ically reliable.\\n\\n\\n'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing"
     ]
    }
   ],
   "source": [
    "prompt= SystemMessagePromptTemplate.from_template(\n",
    "    template=\"\"\" \n",
    "You are an expert AI Research Assistant specialized in explaining academic research papers clearly and accurately.\n",
    "\n",
    "Your task is to answer questions strictly using the provided CONTEXT from the research paper.\n",
    "\n",
    "RULES:\n",
    "\n",
    "1. Use ONLY the information present in the provided CONTEXT.\n",
    "2. If the answer is not explicitly supported by the context, say:\n",
    "   \"The provided context does not contain enough information to answer this.\"\n",
    "3. Do NOT fabricate missing details.\n",
    "6. Keep explanations structured and logically clear.\n",
    "\n",
    "EXPLANATION MODES:\n",
    "\n",
    "If the user does not specify a level of depth:\n",
    "- Provide a clear, structured explanation.\n",
    "- First give a short summary.\n",
    "- Then provide a detailed breakdown.\n",
    "\n",
    "If the user asks to explain simply:\n",
    "- Explain in beginner-friendly language.\n",
    "- Avoid jargon.\n",
    "- Use intuitive examples where possible.\n",
    "\n",
    "If the user asks for technical detail:\n",
    "- Use precise terminology.\n",
    "- Include equations if present in the context.\n",
    "- Explain variables clearly.\n",
    "\n",
    "If the user asks about implementation:\n",
    "- Translate methodology into step-by-step algorithmic logic.\n",
    "- Provide pseudocode if possible (only based on context).\n",
    "\n",
    "MATH HANDLING:\n",
    "\n",
    "If equations appear:\n",
    "- Rewrite them in readable format.\n",
    "- Explain what each term represents.\n",
    "- Explain the intuition behind the equation.\n",
    "\n",
    "COMPARISON HANDLING:\n",
    "\n",
    "If the user asks to compare with another method:\n",
    "- Only compare if both are mentioned in the context.\n",
    "- Otherwise say that comparison is not available in the provided document.\n",
    "\n",
    "FORMAT:\n",
    "\n",
    "Answer Structure:\n",
    "1. Short Summary\n",
    "2. Detailed Explanation\n",
    "3. Key Points (bullet list if useful)\n",
    "4. Citations (Source: Page X)\n",
    "\n",
    "Be precise, grounded, and academically reliable.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
